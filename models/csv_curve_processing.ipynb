{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load JSON\n",
        "# Use absolute path to ensure it works regardless of working directory\n",
        "json_path = '/Users/yashvibhagat/large_exper_unstructured_data/3_combine_jsons_together/combined_papers.json'\n",
        "\n",
        "with open(json_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# print(f\"Successfully loaded data. Number of papers: {len(data.get('papers', []))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of curves in the combined JSON file: 392\n"
          ]
        }
      ],
      "source": [
        "# Count total number of curves across all papers and graphs\n",
        "total_curves = 0\n",
        "for paper in data.get('papers', []):\n",
        "    for graph in paper.get('graphs', []):\n",
        "        total_curves += len(graph.get('curves', []))\n",
        "\n",
        "print(f\"Total number of curves in the combined JSON file: {total_curves}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Extracted 392 curves\n",
            "âœ“ Sample curve keys: ['paper_title', 'graph_id', 'curve_id', 'curve_label', 'x_axis_label', 'y_axis_label', 'y_unit', 'alloy_composition', 'curve_data', 'km_theta0_MPa', 'km_sigma_sat_MPa', 'km_fit_strain_range_min', 'km_fit_strain_range_max', 'km_savgol_window_points', 'km_savgol_poly_order', 'km_goodness_of_fit_R2']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Flatten the nested JSON structure into a list of curves\n",
        "curves_list = []\n",
        "\n",
        "# Loop through all papers\n",
        "for paper in data.get('papers', []):\n",
        "    # For each paper, loop through all graphs\n",
        "    for graph in paper.get('graphs', []):\n",
        "        # Get axis labels WITH units\n",
        "        x_axis_label_full = graph.get('x_axis_label', '')\n",
        "        y_axis_label_full = graph.get('y_axis_label', '')\n",
        "        \n",
        "        # Extract y-axis unit\n",
        "        y_unit = 'unknown'\n",
        "        if 'MPa' in y_axis_label_full:\n",
        "            y_unit = 'MPa'\n",
        "        elif 'GPa' in y_axis_label_full:\n",
        "            y_unit = 'GPa'\n",
        "        \n",
        "        # For each graph, loop through all curves\n",
        "        for curve in graph.get('curves', []):\n",
        "            # Extract Kocks-Mecking hardening parameters\n",
        "            km_params = curve.get('Kocksâ€“Mecking_hardening_parameters', {})\n",
        "            \n",
        "            # Create a dictionary with all information for this curve\n",
        "            curve_info = {\n",
        "                'paper_title': paper.get('paper_title', ''),\n",
        "                'graph_id': graph.get('graph_id', ''),\n",
        "                'curve_id': curve.get('curve_id', ''),\n",
        "                'curve_label': curve.get('curve_label', ''),\n",
        "                'x_axis_label': x_axis_label_full,\n",
        "                'y_axis_label': y_axis_label_full,\n",
        "                'y_unit': y_unit,\n",
        "                'alloy_composition': curve.get('alloy_composition_in_percentages_corresponding_to_this_curve', {}),\n",
        "                'curve_data': curve.get('curve_raw_data', {}).get('data', []),\n",
        "                # Kocks-Mecking Parameters\n",
        "                'km_theta0_MPa': km_params.get('theta0_MPa', None),\n",
        "                'km_sigma_sat_MPa': km_params.get('sigma_sat_MPa', None),\n",
        "                'km_fit_strain_range_min': km_params.get('fit_strain_range', [None, None])[0] if km_params.get('fit_strain_range') else None,\n",
        "                'km_fit_strain_range_max': km_params.get('fit_strain_range', [None, None])[1] if km_params.get('fit_strain_range') else None,\n",
        "                'km_savgol_window_points': km_params.get('savgol_filter', {}).get('window_points', None),\n",
        "                'km_savgol_poly_order': km_params.get('savgol_filter', {}).get('poly_order', None),\n",
        "                'km_goodness_of_fit_R2': km_params.get('goodness_of_fit_R2', None)\n",
        "            }\n",
        "            curves_list.append(curve_info)\n",
        "\n",
        "print(f\"âœ“ Extracted {len(curves_list)} curves\")\n",
        "print(f\"âœ“ Sample curve keys: {list(curves_list[0].keys()) if curves_list else 'No curves found'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            " Unit Conversion\n",
            "============================================================\n",
            "âœ“ Converted from GPa to MPa: 7 curves\n",
            "âœ“ Already in MPa (unchanged): 385 curves\n",
            "âš ï¸  Unknown unit (may need review): 0 curves\n",
            "\n",
            "Total curves processed: 392\n",
            "\n",
            "âœ“ All stress values are now standardized to MPa!\n"
          ]
        }
      ],
      "source": [
        "# STEP 2: Convert stress values from GPa to MPa for consistency\n",
        "# This ensures all stress values are in the same unit (MPa) for future processing\n",
        "\n",
        "converted_count = 0\n",
        "unchanged_count = 0\n",
        "unknown_unit_count = 0\n",
        "\n",
        "for curve_info in curves_list:\n",
        "    y_unit = curve_info.get('y_unit', 'unknown')\n",
        "    curve_data = curve_info.get('curve_data', [])\n",
        "    \n",
        "    if y_unit == 'GPa':\n",
        "        # Convert all y-values from GPa to MPa (multiply by 1000)\n",
        "        for data_point in curve_data:\n",
        "            if 'y' in data_point:\n",
        "                data_point['y'] = data_point['y'] * 1000\n",
        "        \n",
        "        # Update the unit label\n",
        "        curve_info['y_unit'] = 'MPa'\n",
        "        \n",
        "        # Update the y_axis_label to reflect MPa (replace GPa with MPa)\n",
        "        original_label = curve_info.get('y_axis_label', '')\n",
        "        if 'GPa' in original_label:\n",
        "            curve_info['y_axis_label'] = original_label.replace('GPa', 'MPa')\n",
        "        \n",
        "        converted_count += 1\n",
        "        \n",
        "    elif y_unit == 'MPa':\n",
        "        # Already in MPa, no conversion needed\n",
        "        unchanged_count += 1\n",
        "        \n",
        "    else:\n",
        "        # Unknown unit - might need manual review\n",
        "        unknown_unit_count += 1\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\" Unit Conversion\")\n",
        "print(\"=\"*60)\n",
        "print(f\"âœ“ Converted from GPa to MPa: {converted_count} curves\")\n",
        "print(f\"âœ“ Already in MPa (unchanged): {unchanged_count} curves\")\n",
        "print(f\"âš ï¸  Unknown unit (may need review): {unknown_unit_count} curves\")\n",
        "print(f\"\\nTotal curves processed: {len(curves_list)}\")\n",
        "\n",
        "# # Show sample of converted curve\n",
        "# if converted_count > 0:\n",
        "#     # Find first converted curve to show example\n",
        "#     for curve_info in curves_list:\n",
        "#         if 'GPa' not in curve_info.get('y_axis_label', '') and curve_info.get('y_unit') == 'MPa':\n",
        "            # This was likely converted (or already in MPa)\n",
        "            # sample_data = curve_info.get('curve_data', [])[:3]  # First 3 points\n",
        "            # print(f\"\\nðŸ“Š Sample curve (y-axis in MPa):\")\n",
        "            # print(f\"  Curve ID: {curve_info['curve_id']}\")\n",
        "            # print(f\"  Y-axis label: {curve_info['y_axis_label']}\")\n",
        "            # print(f\"  Y-axis unit: {curve_info['y_unit']}\")\n",
        "            # print(f\"  Sample data points (first 3):\")\n",
        "            # for i, point in enumerate(sample_data):\n",
        "            #     print(f\"    Point {i+1}: x={point['x']:.4f}, y={point['y']:.2f} MPa\")\n",
        "            # break\n",
        "\n",
        "print(\"\\nâœ“ All stress values are now standardized to MPa!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ðŸ”„ Step 3: Standardizing Element Names (remove unit suffixes)\n",
            "============================================================\n",
            "âœ“ Standardized compositions: 388 curves\n",
            "âœ“ Total keys before: 1971\n",
            "âœ“ Total keys after: 1971\n",
            "âœ“ Keys renamed: 706\n",
            "\n",
            "============================================================\n",
            "ðŸ”„ Renaming Examples:\n",
            "============================================================\n",
            "  Al:\n",
            "    'Al (at.%)' â†’ 'Al'\n",
            "    'Al (at%)' â†’ 'Al'\n",
            "    'Al_at%' â†’ 'Al'\n",
            "  B:\n",
            "    'B_at%' â†’ 'B'\n",
            "  C:\n",
            "    'C_at%' â†’ 'C'\n",
            "  Co:\n",
            "    'Co (at.%)' â†’ 'Co'\n",
            "    'Co_at%' â†’ 'Co'\n",
            "    'Co (at%)' â†’ 'Co'\n",
            "  Cr:\n",
            "    'Cr (at.%)' â†’ 'Cr'\n",
            "    'Cr_at%' â†’ 'Cr'\n",
            "    'Cr (at%)' â†’ 'Cr'\n",
            "  Cu:\n",
            "    'Cu (at.%)' â†’ 'Cu'\n",
            "    'Cu (at%)' â†’ 'Cu'\n",
            "  Fe:\n",
            "    'Fe (at.%)' â†’ 'Fe'\n",
            "    'Fe_at%' â†’ 'Fe'\n",
            "    'Fe (at%)' â†’ 'Fe'\n",
            "  Hf:\n",
            "    'Hf_at%' â†’ 'Hf'\n",
            "  Mg:\n",
            "    'Mg (wt.%)' â†’ 'Mg'\n",
            "  Mn:\n",
            "    'Mn (at.%)' â†’ 'Mn'\n",
            "    'Mn_at%' â†’ 'Mn'\n",
            "\n",
            "============================================================\n",
            "ðŸ“‹ All Unique Alloy Composition Column Names (30 total)\n",
            "============================================================\n",
            "['Al', 'B', 'C', 'Co', 'Cr', 'Cu', 'Fe', 'Hf', 'Mg', 'Mn', 'Mo', 'N', 'Nb', 'NbPlusTa', 'Nb_Ta', 'Nd', 'Ni', 'Si', 'Ta', 'Ti', 'V', 'W', 'Y', 'Zr', '_note', 'basis', 'nominal', 'note', 'unit', 'units']\n",
            "\n",
            "âœ“ Standardization complete!\n"
          ]
        }
      ],
      "source": [
        "# STEP 3: Standardize alloy composition names (remove _at%, (at%), (wt%), etc.)\n",
        "# This step happens after GPa to MPa conversion and before wt% to at% conversion\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ðŸ”„ Step 3: Standardizing Element Names (remove unit suffixes)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def get_base_element(key):\n",
        "    \"\"\"\n",
        "    Extract base element name from key, removing all unit indicators.\n",
        "    Examples:\n",
        "    - 'Al' -> 'Al'\n",
        "    - 'Al_at%' -> 'Al'\n",
        "    - 'Al (at%)' -> 'Al'\n",
        "    - 'Al (at.%), if some wt.%)' -> 'Al'\n",
        "    - 'Zr (wt.%)' -> 'Zr'\n",
        "    - 'Co_at%' -> 'Co'\n",
        "    - 'Fe (wt%)' -> 'Fe'\n",
        "    \"\"\"\n",
        "    import re\n",
        "    \n",
        "    # Remove everything after '(' including the parenthesis\n",
        "    # This handles: 'Al (at%)', 'Al (at.%), if some wt.%)', 'Zr (wt.%)'\n",
        "    base = re.sub(r'\\s*\\(.*$', '', key)\n",
        "    \n",
        "    # Remove _at% and _wt% suffixes\n",
        "    base = base.replace('_at%', '').replace('_wt%', '')\n",
        "    base = base.replace('_at', '').replace('_wt', '')\n",
        "    \n",
        "    # Remove any trailing whitespace\n",
        "    base = base.strip()\n",
        "    \n",
        "    return base\n",
        "\n",
        "standardized_count = 0\n",
        "keys_renamed = 0\n",
        "total_keys_before = 0\n",
        "total_keys_after = 0\n",
        "\n",
        "# Track what was renamed for reporting\n",
        "rename_examples = {}\n",
        "\n",
        "for curve_info in curves_list:\n",
        "    composition = curve_info.get('alloy_composition', {})\n",
        "    \n",
        "    if not composition:\n",
        "        continue\n",
        "    \n",
        "    total_keys_before += len(composition)\n",
        "    \n",
        "    # Standardize element names\n",
        "    standardized_composition = {}\n",
        "    \n",
        "    for key, value in composition.items():\n",
        "        base_element = get_base_element(key)\n",
        "        \n",
        "        # Track if we renamed this key\n",
        "        if key != base_element:\n",
        "            keys_renamed += 1\n",
        "            # Store example of what was renamed\n",
        "            if base_element not in rename_examples:\n",
        "                rename_examples[base_element] = []\n",
        "            if key not in rename_examples[base_element]:\n",
        "                rename_examples[base_element].append(key)\n",
        "        \n",
        "        standardized_composition[base_element] = value\n",
        "    \n",
        "    # Update composition with standardized version\n",
        "    curve_info['alloy_composition'] = standardized_composition\n",
        "    \n",
        "    total_keys_after += len(standardized_composition)\n",
        "    \n",
        "    if len(standardized_composition) > 0:\n",
        "        standardized_count += 1\n",
        "\n",
        "print(f\"âœ“ Standardized compositions: {standardized_count} curves\")\n",
        "print(f\"âœ“ Total keys before: {total_keys_before}\")\n",
        "print(f\"âœ“ Total keys after: {total_keys_after}\")\n",
        "print(f\"âœ“ Keys renamed: {keys_renamed}\")\n",
        "\n",
        "# Show what was renamed\n",
        "if rename_examples:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ðŸ”„ Renaming Examples:\")\n",
        "    print(\"=\"*60)\n",
        "    for base_element, original_keys in sorted(rename_examples.items())[:10]:\n",
        "        print(f\"  {base_element}:\")\n",
        "        for orig in original_keys[:3]:  # Show first 3 examples\n",
        "            print(f\"    '{orig}' â†’ '{base_element}'\")\n",
        "\n",
        "# Collect all unique column names after standardization\n",
        "all_column_names = set()\n",
        "for curve_info in curves_list:\n",
        "    composition = curve_info.get('alloy_composition', {})\n",
        "    if composition:\n",
        "        all_column_names.update(composition.keys())\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"ðŸ“‹ All Unique Alloy Composition Column Names ({len(all_column_names)} total)\")\n",
        "print(\"=\"*60)\n",
        "print(sorted(all_column_names))\n",
        "\n",
        "print(\"\\nâœ“ Standardization complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Step 4: Removing Unnecessary Columns\n",
            "============================================================\n",
            "âœ“ Removed 54 unwanted column entries\n",
            "âœ“ Affected 51 curves\n",
            "\n",
            "Total curves processed: 392\n"
          ]
        }
      ],
      "source": [
        "# STEP 4: Remove unnecessary columns from alloy compositions\n",
        "# This happens after standardization\n",
        "\n",
        "# Columns to remove\n",
        "COLUMNS_TO_REMOVE = ['NbPlusTa', 'Nb_Ta', '_note', 'basis', 'nominal', 'note', 'unit', 'units']\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Step 4: Removing Unnecessary Columns\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "total_removed = 0\n",
        "curves_affected = 0\n",
        "\n",
        "for curve_info in curves_list:\n",
        "    raw_composition = curve_info.get('alloy_composition', {})\n",
        "    \n",
        "    if not raw_composition:\n",
        "        continue\n",
        "    \n",
        "    # Create cleaned composition without unwanted columns\n",
        "    cleaned_composition = {}\n",
        "    removed_count = 0\n",
        "    \n",
        "    for key, value in raw_composition.items():\n",
        "        if key not in COLUMNS_TO_REMOVE:\n",
        "            cleaned_composition[key] = value\n",
        "        else:\n",
        "            removed_count += 1\n",
        "    \n",
        "    # Update the composition\n",
        "    curve_info['alloy_composition'] = cleaned_composition\n",
        "    \n",
        "    if removed_count > 0:\n",
        "        curves_affected += 1\n",
        "        total_removed += removed_count\n",
        "\n",
        "print(f\"âœ“ Removed {total_removed} unwanted column entries\")\n",
        "print(f\"âœ“ Affected {curves_affected} curves\")\n",
        "print(f\"\\nTotal curves processed: {len(curves_list)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ðŸ”¬ Step 5: Converting wt% to at%\n",
            "============================================================\n",
            "âœ“ Converted from wt% to at%: 382 curves\n",
            "âœ“ Already in at%: 0 curves\n",
            "âš ï¸  Mixed units (converted wt% portion): 0 curves\n",
            "\n",
            "Total curves processed: 392\n"
          ]
        }
      ],
      "source": [
        "# STEP 5: Convert wt% to at%\n",
        "\n",
        "import re\n",
        "\n",
        "# Standard atomic weights (g/mol) for common elements in high-entropy alloys\n",
        "ATOMIC_WEIGHTS = {\n",
        "    'Al': 26.982,\n",
        "    'Co': 58.933,\n",
        "    'Cr': 51.996,\n",
        "    'Cu': 63.546,\n",
        "    'Fe': 55.845,\n",
        "    'Mn': 54.938,\n",
        "    'Mo': 95.95,\n",
        "    'Nb': 92.906,\n",
        "    'Ni': 58.693,\n",
        "    'Ta': 180.948,\n",
        "    'Ti': 47.867,\n",
        "    'V': 50.942,\n",
        "    'W': 183.84,\n",
        "    'Zr': 91.224,\n",
        "    'C': 12.011,\n",
        "    'N': 14.007,\n",
        "    'O': 15.999,\n",
        "    'Si': 28.085,\n",
        "    'Mg': 24.305,\n",
        "    'Zn': 65.38,\n",
        "    'Sn': 118.71,\n",
        "    'Hf': 178.49,\n",
        "    'Re': 186.207,\n",
        "    'Ru': 101.07,\n",
        "    'Pd': 106.42,\n",
        "    'Pt': 195.084,\n",
        "    'Au': 196.967,\n",
        "    'Ag': 107.868,\n",
        "    'B': 10.81,\n",
        "    'P': 30.974,\n",
        "    'S': 32.06,\n",
        "}\n",
        "\n",
        "def extract_numeric_value(value):\n",
        "    \"\"\"\n",
        "    Extract numeric value from string, handling various formats:\n",
        "    - 'â‰¥10 (locally enriched due to added powders)' -> 10.0\n",
        "    - '5-10' -> 7.5 (average)\n",
        "    - '~5' -> 5.0\n",
        "    - '<0.1' -> 0.1\n",
        "    - 'balance' -> None\n",
        "    \"\"\"\n",
        "    if value is None:\n",
        "        return None\n",
        "    \n",
        "    # If already a number, return it\n",
        "    if isinstance(value, (int, float)):\n",
        "        return float(value)\n",
        "    \n",
        "    # Convert to string\n",
        "    value_str = str(value).strip().lower()\n",
        "    \n",
        "    # Handle special cases\n",
        "    if value_str in ['balance', 'bal', 'bal.', 'remainder', 'rem', 'rest']:\n",
        "        return None  # Will calculate balance later\n",
        "    \n",
        "    if value_str in ['trace', 'traces', '-', '', 'n/a', 'na']:\n",
        "        return 0.0\n",
        "    \n",
        "    # Extract all numbers from the string using regex\n",
        "    numbers = re.findall(r'[-+]?\\d*\\.?\\d+', value_str)\n",
        "    \n",
        "    if not numbers:\n",
        "        return None\n",
        "    \n",
        "    # Convert to floats\n",
        "    numbers = [float(n) for n in numbers]\n",
        "    \n",
        "    # Handle ranges (e.g., \"5-10\" or \"5 to 10\")\n",
        "    if len(numbers) >= 2:\n",
        "        # Take average of range\n",
        "        return sum(numbers) / len(numbers)\n",
        "    \n",
        "    # Single number\n",
        "    if len(numbers) == 1:\n",
        "        return numbers[0]\n",
        "    \n",
        "    return None\n",
        "\n",
        "def extract_element_from_key(key):\n",
        "    \"\"\"\n",
        "    Extract the element symbol from a key.\n",
        "    Examples: 'Al_wt%' -> 'Al', 'Cr(wt%)' -> 'Cr', 'Fe' -> 'Fe'\n",
        "    \"\"\"\n",
        "    # Remove unit indicators\n",
        "    clean = key.replace('_wt%', '').replace('_at%', '').replace('_wt', '').replace('_at', '')\n",
        "    clean = clean.replace('(wt%)', '').replace('(at%)', '').replace('(wt)', '').replace('(at)', '')\n",
        "    clean = clean.replace(' wt%', '').replace(' at%', '').replace(' wt', '').replace(' at', '')\n",
        "    clean = clean.replace('wt%', '').replace('at%', '')\n",
        "    clean = clean.replace('(', '').replace(')', '')\n",
        "    clean = clean.strip()\n",
        "    return clean\n",
        "\n",
        "def is_wt_percent_key(key):\n",
        "    \"\"\"Check if key indicates weight percent\"\"\"\n",
        "    key_lower = key.lower()\n",
        "    return 'wt%' in key_lower or 'wt %' in key_lower or '(wt)' in key_lower or '_wt' in key_lower\n",
        "\n",
        "def is_at_percent_key(key):\n",
        "    \"\"\"Check if key indicates atomic percent\"\"\"\n",
        "    key_lower = key.lower()\n",
        "    return 'at%' in key_lower or 'at %' in key_lower or '(at)' in key_lower or '_at' in key_lower\n",
        "\n",
        "def convert_wt_to_at(composition_wt_dict):\n",
        "    \"\"\"\n",
        "    Convert weight percent to atomic percent.\n",
        "    \n",
        "    Formula:\n",
        "    at% = (wt% / atomic_weight) / sum(wt% / atomic_weight for all elements) * 100\n",
        "    \n",
        "    Args:\n",
        "        composition_wt_dict: dict with {element: wt%}\n",
        "    \n",
        "    Returns:\n",
        "        dict with {element: at%}\n",
        "    \"\"\"\n",
        "    if not composition_wt_dict:\n",
        "        return {}\n",
        "    \n",
        "    # Calculate mole fractions\n",
        "    mole_fractions = {}\n",
        "    total_moles = 0\n",
        "    balance_element = None\n",
        "    \n",
        "    for element, wt_percent in composition_wt_dict.items():\n",
        "        if wt_percent is None:\n",
        "            balance_element = element\n",
        "            continue\n",
        "        \n",
        "        if element in ATOMIC_WEIGHTS and wt_percent > 0:\n",
        "            moles = wt_percent / ATOMIC_WEIGHTS[element]\n",
        "            mole_fractions[element] = moles\n",
        "            total_moles += moles\n",
        "    \n",
        "    # Handle balance element\n",
        "    if balance_element and balance_element in ATOMIC_WEIGHTS:\n",
        "        total_wt = sum(v for v in composition_wt_dict.values() if v is not None)\n",
        "        if total_wt < 100:\n",
        "            balance_wt = 100 - total_wt\n",
        "            moles = balance_wt / ATOMIC_WEIGHTS[balance_element]\n",
        "            mole_fractions[balance_element] = moles\n",
        "            total_moles += moles\n",
        "    \n",
        "    # Convert to atomic percent\n",
        "    composition_at = {}\n",
        "    if total_moles > 0:\n",
        "        for element, moles in mole_fractions.items():\n",
        "            composition_at[element] = (moles / total_moles) * 100\n",
        "    \n",
        "    return composition_at\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ðŸ”¬ Step 5: Converting wt% to at%\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "converted_count = 0\n",
        "already_at_count = 0\n",
        "mixed_units_count = 0\n",
        "parse_errors = []\n",
        "\n",
        "for idx, curve_info in enumerate(curves_list):\n",
        "    composition = curve_info.get('alloy_composition', {})\n",
        "    \n",
        "    if not composition:\n",
        "        continue\n",
        "    \n",
        "    try:\n",
        "        # Separate by unit type\n",
        "        wt_percent_data = {}\n",
        "        at_percent_data = {}\n",
        "        unknown_data = {}\n",
        "        \n",
        "        for key, value in composition.items():\n",
        "            element = extract_element_from_key(key)\n",
        "            \n",
        "            # Skip if element not recognized\n",
        "            if not element:\n",
        "                continue\n",
        "            \n",
        "            # Parse the value\n",
        "            numeric_value = extract_numeric_value(value)\n",
        "            \n",
        "            # Classify by unit type\n",
        "            if is_at_percent_key(key):\n",
        "                at_percent_data[element] = numeric_value\n",
        "            elif is_wt_percent_key(key):\n",
        "                wt_percent_data[element] = numeric_value\n",
        "            else:\n",
        "                # No unit specified - assume wt% (common convention)\n",
        "                unknown_data[element] = numeric_value\n",
        "        \n",
        "        # Determine what conversion is needed\n",
        "        has_wt = len(wt_percent_data) > 0 or len(unknown_data) > 0\n",
        "        has_at = len(at_percent_data) > 0\n",
        "        \n",
        "        # Convert wt% to at%\n",
        "        converted_composition = {}\n",
        "        \n",
        "        if has_wt and not has_at:\n",
        "            # All wt%, convert everything\n",
        "            all_wt = {**wt_percent_data, **unknown_data}\n",
        "            converted_composition = convert_wt_to_at(all_wt)\n",
        "            converted_count += 1\n",
        "            \n",
        "        elif has_at and not has_wt:\n",
        "            # All at%, no conversion needed\n",
        "            converted_composition = at_percent_data\n",
        "            already_at_count += 1\n",
        "            \n",
        "        elif has_wt and has_at:\n",
        "            # Mixed units - convert wt% and combine with at%\n",
        "            wt_converted = convert_wt_to_at({**wt_percent_data, **unknown_data})\n",
        "            converted_composition = {**at_percent_data, **wt_converted}\n",
        "            mixed_units_count += 1\n",
        "        \n",
        "        # Store the converted composition\n",
        "        curve_info['alloy_composition_converted'] = converted_composition\n",
        "        \n",
        "    except Exception as e:\n",
        "        parse_errors.append({\n",
        "            'curve_id': curve_info.get('curve_id'),\n",
        "            'error': str(e),\n",
        "            'composition': composition\n",
        "        })\n",
        "        curve_info['alloy_composition_converted'] = {}\n",
        "\n",
        "print(f\"âœ“ Converted from wt% to at%: {converted_count} curves\")\n",
        "print(f\"âœ“ Already in at%: {already_at_count} curves\")\n",
        "print(f\"âš ï¸  Mixed units (converted wt% portion): {mixed_units_count} curves\")\n",
        "\n",
        "print(f\"\\nTotal curves processed: {len(curves_list)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 5: Creating Composition String Target\n",
            "============================================================\n",
            "âœ“ Curves with composition string: 382\n",
            "âœ“ Curves without composition: 10\n",
            "\n",
            "ðŸ“Š Statistics:\n",
            "  Total unique alloy compositions: 77\n",
            "  Total curves processed: 392\n",
            "\n",
            "============================================================\n",
            "ðŸ“‹ Sample Composition Strings (First 10):\n",
            "============================================================\n",
            "\n",
            "1. Curve ID: fig14a-77K\n",
            "   Dict: {'Co': 20.0, 'Cr': 20.0, 'Fe': 20.0, 'Mn': 20.0, 'Ni': 20.0}\n",
            "   String: Co20-Cr20-Fe20-Mn20-Ni20\n",
            "\n",
            "2. Curve ID: fig14a-293K\n",
            "   Dict: {'Co': 20.0, 'Cr': 20.0, 'Fe': 20.0, 'Mn': 20.0, 'Ni': 20.0}\n",
            "   String: Co20-Cr20-Fe20-Mn20-Ni20\n",
            "\n",
            "3. Curve ID: fig14a-473K\n",
            "   Dict: {'Co': 20.0, 'Cr': 20.0, 'Fe': 20.0, 'Mn': 20.0, 'Ni': 20.0}\n",
            "   String: Co20-Cr20-Fe20-Mn20-Ni20\n",
            "\n",
            "4. Curve ID: fig14a-673K\n",
            "   Dict: {'Co': 20.0, 'Cr': 20.0, 'Fe': 20.0, 'Mn': 20.0, 'Ni': 20.0}\n",
            "   String: Co20-Cr20-Fe20-Mn20-Ni20\n",
            "\n",
            "5. Curve ID: fig14a-873K\n",
            "   Dict: {'Co': 20.0, 'Cr': 20.0, 'Fe': 20.0, 'Mn': 20.0, 'Ni': 20.0}\n",
            "   String: Co20-Cr20-Fe20-Mn20-Ni20\n",
            "\n",
            "6. Curve ID: fig14a-1073K\n",
            "   Dict: {'Co': 20.0, 'Cr': 20.0, 'Fe': 20.0, 'Mn': 20.0, 'Ni': 20.0}\n",
            "   String: Co20-Cr20-Fe20-Mn20-Ni20\n",
            "\n",
            "7. Curve ID: fig14b-298K\n",
            "   Dict: {'Al': 2.44, 'Co': 24.39, 'Cr': 24.39, 'Fe': 24.39, 'Ni': 24.39}\n",
            "   String: Al2.44-Co24.39-Cr24.39-Fe24.39-Ni24.39\n",
            "\n",
            "8. Curve ID: fig14b-200K\n",
            "   Dict: {'Al': 2.44, 'Co': 24.39, 'Cr': 24.39, 'Fe': 24.39, 'Ni': 24.39}\n",
            "   String: Al2.44-Co24.39-Cr24.39-Fe24.39-Ni24.39\n",
            "\n",
            "9. Curve ID: fig14b-77K\n",
            "   Dict: {'Al': 2.44, 'Co': 24.39, 'Cr': 24.39, 'Fe': 24.39, 'Ni': 24.39}\n",
            "   String: Al2.44-Co24.39-Cr24.39-Fe24.39-Ni24.39\n",
            "\n",
            "10. Curve ID: fig15a-295K\n",
            "   Dict: {'Cr': 20.0, 'Mn': 20.0, 'Fe': 20.0, 'Co': 20.0, 'Ni': 20.0}\n",
            "   String: Co20-Cr20-Fe20-Mn20-Ni20\n",
            "\n",
            "============================================================\n",
            "ðŸ”¬ Sample of Unique Compositions (First 20):\n",
            "============================================================\n",
            "1. Al1-Co20-Cr15-Fe39-Mn20-Si5\n",
            "2. Al10-Co17-Cr24-Fe19-Ni29\n",
            "3. Al10-Co22.5-Cr22.5-Fe22.5-Ni22.5\n",
            "4. Al10.7-B2.5-Co22.4-Fe8.8-Ni43.9-Ti11.7\n",
            "5. Al14.89-Co21.28-Cr21.28-Fe21.28-Ni21.28\n",
            "6. Al14.9-Co16.2-Cr18.1-Fe17.3-Ni32.8-Ti0.7\n",
            "7. Al16-Co16.1-Cr18-Fe17.1-Ni32.8\n",
            "8. Al16.16-Co17.07-Cr15.86-Cu17.42-Fe15.96-Ni16.65\n",
            "9. Al16.39-Co16.39-Cr16.39-Fe16.39-Ni34.43\n",
            "10. Al16.39-Co16.39-Cr16.39-Fe16.39-Ni34.44\n",
            "11. Al16.4-Co16.4-Cr16.4-Fe16.4-Ni34.4\n",
            "12. Al16.67-Co16.67-Cr16.67-Cu16.67-Fe16.67-Ni16.67\n",
            "13. Al2.4-Co24.4-Cr24.4-Fe24.4-Ni24.4\n",
            "14. Al2.44-Co24.39-Cr24.39-Fe24.39-Ni24.39\n",
            "15. Al24.38-B2.5-Ni73.12\n",
            "16. Al25-Nb25-Ti25-V25\n",
            "17. Al3.23-Co32.26-Cr32.26-Ni32.26\n",
            "18. Al3.5-B0.03-C0.2-Fe72.67-Ni23-Ti0.5-Zr0.1\n",
            "19. Al4.76-Co23.81-Cr23.81-Fe23.81-Ni23.81\n",
            "20. Al5-Cr20-Fe35-Mo5-Ni35\n",
            "\n",
            "============================================================\n",
            "âœ… PREPROCESSING COMPLETE!\n",
            "============================================================\n",
            "Total curves: 392\n",
            "Unique compositions: 77\n",
            "Curves with target string: 382\n",
            "\n",
            "Your target variable 'composition_string' is ready with NO NaN values!\n",
            "Format: 'Co10-Cr10-Fe49-Mn30-N1' (alphabetically sorted)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 5: Creating Composition String Target\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def composition_to_string(composition_dict):\n",
        "    \"\"\"\n",
        "    Convert composition dictionary to standardized string.\n",
        "    Elements are sorted alphabetically and concatenated as \"ElementPercent\".\n",
        "    \n",
        "    Example: {\"Fe\":49, \"Mn\":30, \"Co\":10, \"Cr\":10, \"N\":1} \n",
        "    Returns: \"Co10-Cr10-Fe49-Mn30-N1\"\n",
        "    \n",
        "    Rules:\n",
        "    - Alphabetically sorted by element name\n",
        "    - Format: Element + rounded percentage\n",
        "    - Separated by hyphens\n",
        "    - Skip elements with None or 0 values\n",
        "    \"\"\"\n",
        "    if not composition_dict:\n",
        "        return \"\"\n",
        "    \n",
        "    # Filter out None and 0 values, convert to numeric\n",
        "    valid_elements = {}\n",
        "    for element, value in composition_dict.items():\n",
        "        # Try to convert value to float\n",
        "        try:\n",
        "            if value is not None:\n",
        "                numeric_value = float(value)\n",
        "                if numeric_value > 0:\n",
        "                    # Round to 2 decimal places\n",
        "                    rounded = round(numeric_value, 2)\n",
        "                    # Remove .0 if it's a whole number\n",
        "                    if rounded == int(rounded):\n",
        "                        valid_elements[element] = int(rounded)\n",
        "                    else:\n",
        "                        valid_elements[element] = rounded\n",
        "        except (ValueError, TypeError):\n",
        "            # Skip values that can't be converted to float\n",
        "            continue\n",
        "    \n",
        "    # Sort alphabetically and create string\n",
        "    sorted_elements = sorted(valid_elements.items())\n",
        "    \n",
        "    # Create string like \"Co10-Cr10-Fe49-Mn30-N1\"\n",
        "    composition_string = \"-\".join([f\"{elem}{val}\" for elem, val in sorted_elements])\n",
        "    \n",
        "    return composition_string\n",
        "\n",
        "# Create composition strings for all curves\n",
        "curves_with_composition = 0\n",
        "curves_without_composition = 0\n",
        "\n",
        "for curve_info in curves_list:\n",
        "    composition = curve_info.get('alloy_composition', {})\n",
        "    \n",
        "    if composition:\n",
        "        comp_string = composition_to_string(composition)\n",
        "        curve_info['composition_string'] = comp_string\n",
        "        if comp_string:\n",
        "            curves_with_composition += 1\n",
        "        else:\n",
        "            curves_without_composition += 1\n",
        "    else:\n",
        "        curve_info['composition_string'] = \"\"\n",
        "        curves_without_composition += 1\n",
        "\n",
        "print(f\"âœ“ Curves with composition string: {curves_with_composition}\")\n",
        "print(f\"âœ“ Curves without composition: {curves_without_composition}\")\n",
        "\n",
        "# Analyze unique compositions\n",
        "unique_compositions = set()\n",
        "for curve_info in curves_list:\n",
        "    comp_string = curve_info.get('composition_string', '')\n",
        "    if comp_string:\n",
        "        unique_compositions.add(comp_string)\n",
        "\n",
        "print(f\"\\nðŸ“Š Statistics:\")\n",
        "print(f\"  Total unique alloy compositions: {len(unique_compositions)}\")\n",
        "print(f\"  Total curves processed: {len(curves_list)}\")\n",
        "\n",
        "# Show first 10 examples\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ“‹ Sample Composition Strings (First 10):\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, curve_info in enumerate(curves_list[:10]):\n",
        "    comp_dict = curve_info.get('alloy_composition', {})\n",
        "    comp_string = curve_info.get('composition_string', '')\n",
        "    \n",
        "    print(f\"\\n{i+1}. Curve ID: {curve_info['curve_id']}\")\n",
        "    print(f\"   Dict: {comp_dict}\")\n",
        "    print(f\"   String: {comp_string}\")\n",
        "\n",
        "# Show some unique compositions\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ”¬ Sample of Unique Compositions (First 20):\")\n",
        "print(\"=\"*60)\n",
        "for i, comp in enumerate(sorted(list(unique_compositions))[:20]):\n",
        "    print(f\"{i+1}. {comp}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… PREPROCESSING COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total curves: {len(curves_list)}\")\n",
        "print(f\"Unique compositions: {len(unique_compositions)}\")\n",
        "print(f\"Curves with target string: {curves_with_composition}\")\n",
        "print(\"\\nYour target variable 'composition_string' is ready with NO NaN values!\")\n",
        "print(\"Format: 'Co10-Cr10-Fe49-Mn30-N1' (alphabetically sorted)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Feature extraction function defined! (54 features)\n",
            "  Ready to extract features from curve_data in next step.\n"
          ]
        }
      ],
      "source": [
        "# STEP 6A: Define feature extraction function\n",
        "# This function extracts 54 comprehensive features from stress-strain curve data\n",
        "\n",
        "import numpy as np\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.signal import savgol_filter\n",
        "from scipy.integrate import trapezoid\n",
        "\n",
        "def extract_features_from_curve(curve_data, debug=False):\n",
        "    \"\"\"\n",
        "    Extract COMPREHENSIVE features from a stress-strain curve.\n",
        "    Returns 54 features covering mechanical properties, statistics, and curve shape.\n",
        "    \"\"\"\n",
        "    # === STEP 1: Check if data is valid ===\n",
        "    if not curve_data or len(curve_data) < 6:\n",
        "        if debug:\n",
        "            print(\"âš ï¸ Skipping: Not enough data points\")\n",
        "        return None\n",
        "    \n",
        "    # === STEP 2: Convert to arrays ===\n",
        "    strains_unsorted = np.array([p['x'] for p in curve_data])\n",
        "    stresses_unsorted = np.array([p['y'] for p in curve_data])\n",
        "    \n",
        "    # === STEP 3: Sort by strain ===\n",
        "    sort_idx = np.argsort(strains_unsorted)\n",
        "    strains = strains_unsorted[sort_idx]\n",
        "    stresses = stresses_unsorted[sort_idx]\n",
        "    \n",
        "    # Initialize features dictionary\n",
        "    features = {}\n",
        "    \n",
        "    # === STEP 4: Basic Mechanical Properties ===\n",
        "    features['ultimate_tensile_strength'] = float(np.max(stresses))\n",
        "    features['max_strain'] = float(np.max(strains))\n",
        "    features['uts_strain'] = float(strains[np.argmax(stresses)])\n",
        "    features['min_stress'] = float(np.min(stresses))\n",
        "    features['min_strain'] = float(np.min(strains))\n",
        "    \n",
        "    # === STEP 5: Yield Strength (0.2% offset approximation) ===\n",
        "    if strains[0] <= 0.002 <= strains[-1]:\n",
        "        f_interp = interp1d(strains, stresses, kind='linear', fill_value='extrapolate', bounds_error=False)\n",
        "        features['yield_strength_002'] = float(f_interp(0.002))\n",
        "    else:\n",
        "        features['yield_strength_002'] = float(stresses[0])\n",
        "    \n",
        "    features['yield_to_uts_ratio'] = float(features['yield_strength_002'] / (features['ultimate_tensile_strength'] + 1e-10))\n",
        "    \n",
        "    # === STEP 6: Stress at specific strain points ===\n",
        "    strain_points = [0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.5]\n",
        "    for sp in strain_points:\n",
        "        if strains[0] <= sp <= strains[-1]:\n",
        "            f_interp = interp1d(strains, stresses, kind='linear', fill_value='extrapolate', bounds_error=False)\n",
        "            features[f'stress_at_{sp}'] = float(f_interp(sp))\n",
        "        else:\n",
        "            features[f'stress_at_{sp}'] = np.nan\n",
        "    \n",
        "    # === STEP 7: Elastic Modulus (Young's Modulus) ===\n",
        "    elastic_portion = int(len(strains) * 0.2)\n",
        "    if elastic_portion > 2:\n",
        "        elastic_strains = strains[:elastic_portion]\n",
        "        elastic_stresses = stresses[:elastic_portion]\n",
        "        if len(elastic_strains) > 1 and (elastic_strains[-1] - elastic_strains[0]) > 1e-6:\n",
        "            E_fit = np.polyfit(elastic_strains, elastic_stresses, 1)\n",
        "            features['elastic_modulus'] = float(E_fit[0])\n",
        "            features['elastic_intercept'] = float(E_fit[1])\n",
        "        else:\n",
        "            features['elastic_modulus'] = np.nan\n",
        "            features['elastic_intercept'] = np.nan\n",
        "    else:\n",
        "        features['elastic_modulus'] = np.nan\n",
        "        features['elastic_intercept'] = np.nan\n",
        "    \n",
        "    # === STEP 8: Strain Hardening Exponent (n-value) ===\n",
        "    plastic_mask = strains > 0.002\n",
        "    if np.sum(plastic_mask) > 5:\n",
        "        plastic_strains = strains[plastic_mask]\n",
        "        plastic_stresses = stresses[plastic_mask]\n",
        "        valid_mask = (plastic_strains > 0) & (plastic_stresses > 0)\n",
        "        if np.sum(valid_mask) > 3:\n",
        "            try:\n",
        "                log_strain = np.log(plastic_strains[valid_mask])\n",
        "                log_stress = np.log(plastic_stresses[valid_mask])\n",
        "                n_fit = np.polyfit(log_strain, log_stress, 1)\n",
        "                features['strain_hardening_exponent'] = float(n_fit[0])\n",
        "                features['strength_coefficient_K'] = float(np.exp(n_fit[1]))\n",
        "            except:\n",
        "                features['strain_hardening_exponent'] = np.nan\n",
        "                features['strength_coefficient_K'] = np.nan\n",
        "        else:\n",
        "            features['strain_hardening_exponent'] = np.nan\n",
        "            features['strength_coefficient_K'] = np.nan\n",
        "    else:\n",
        "        features['strain_hardening_exponent'] = np.nan\n",
        "        features['strength_coefficient_K'] = np.nan\n",
        "    \n",
        "    # === STEP 9: Work Hardening Analysis ===\n",
        "    if len(strains) > 5:\n",
        "        window = min(5, len(stresses) if len(stresses) % 2 == 1 else len(stresses) - 1)\n",
        "        if window >= 3:\n",
        "            stresses_smooth = savgol_filter(stresses, window, 2)\n",
        "        else:\n",
        "            stresses_smooth = stresses\n",
        "        \n",
        "        d_stress = np.diff(stresses_smooth)\n",
        "        d_strain = np.diff(strains)\n",
        "        hardening_rate = d_stress / (d_strain + 1e-10)\n",
        "        \n",
        "        features['avg_hardening_rate'] = float(np.mean(hardening_rate))\n",
        "        features['max_hardening_rate'] = float(np.max(hardening_rate))\n",
        "        features['min_hardening_rate'] = float(np.min(hardening_rate))\n",
        "        features['std_hardening_rate'] = float(np.std(hardening_rate))\n",
        "        \n",
        "        n_points = len(hardening_rate)\n",
        "        if n_points >= 3:\n",
        "            features['hardening_rate_early'] = float(np.mean(hardening_rate[:n_points//3]))\n",
        "            features['hardening_rate_mid'] = float(np.mean(hardening_rate[n_points//3:2*n_points//3]))\n",
        "            features['hardening_rate_late'] = float(np.mean(hardening_rate[2*n_points//3:]))\n",
        "        else:\n",
        "            features['hardening_rate_early'] = float(np.mean(hardening_rate))\n",
        "            features['hardening_rate_mid'] = float(np.mean(hardening_rate))\n",
        "            features['hardening_rate_late'] = float(np.mean(hardening_rate))\n",
        "        \n",
        "        features['hardening_rate_ratio_early_mid'] = float(features['hardening_rate_early'] / (features['hardening_rate_mid'] + 1e-10))\n",
        "        features['hardening_rate_ratio_mid_late'] = float(features['hardening_rate_mid'] / (features['hardening_rate_late'] + 1e-10))\n",
        "    \n",
        "    # === STEP 10: Energy Metrics ===\n",
        "    features['toughness'] = float(trapezoid(stresses, strains))\n",
        "    elastic_idx = int(len(strains) * 0.2)\n",
        "    if elastic_idx > 1:\n",
        "        features['resilience'] = float(trapezoid(stresses[:elastic_idx], strains[:elastic_idx]))\n",
        "    else:\n",
        "        features['resilience'] = 0.0\n",
        "    \n",
        "    features['energy_elastic_region'] = float(trapezoid(stresses[:elastic_idx], strains[:elastic_idx]) if elastic_idx > 1 else 0)\n",
        "    features['energy_plastic_region'] = float(trapezoid(stresses[elastic_idx:], strains[elastic_idx:]) if len(strains) > elastic_idx else 0)\n",
        "    features['plastic_to_elastic_energy'] = float(features['energy_plastic_region'] / (features['energy_elastic_region'] + 1e-10))\n",
        "    \n",
        "    # === STEP 11: Statistical Features ===\n",
        "    features['stress_mean'] = float(np.mean(stresses))\n",
        "    features['stress_std'] = float(np.std(stresses))\n",
        "    features['stress_median'] = float(np.median(stresses))\n",
        "    features['stress_25_percentile'] = float(np.percentile(stresses, 25))\n",
        "    features['stress_75_percentile'] = float(np.percentile(stresses, 75))\n",
        "    features['stress_range'] = float(np.max(stresses) - np.min(stresses))\n",
        "    features['stress_cv'] = float(features['stress_std'] / (features['stress_mean'] + 1e-10))\n",
        "    \n",
        "    features['strain_mean'] = float(np.mean(strains))\n",
        "    features['strain_std'] = float(np.std(strains))\n",
        "    \n",
        "    # === STEP 12: Curve Shape Features ===\n",
        "    features['num_data_points'] = len(strains)\n",
        "    features['strain_range'] = float(np.max(strains) - np.min(strains))\n",
        "    \n",
        "    n = len(strains)\n",
        "    if n >= 4:\n",
        "        early_idx = max(1, n // 4)\n",
        "        early_slope = (stresses[early_idx] - stresses[0]) / (strains[early_idx] - strains[0] + 1e-10)\n",
        "        features['early_slope'] = float(early_slope)\n",
        "        \n",
        "        mid_idx = n // 2\n",
        "        mid_slope = (stresses[mid_idx] - stresses[early_idx]) / (strains[mid_idx] - strains[early_idx] + 1e-10)\n",
        "        features['mid_slope'] = float(mid_slope)\n",
        "        \n",
        "        late_slope = (stresses[-1] - stresses[mid_idx]) / (strains[-1] - strains[mid_idx] + 1e-10)\n",
        "        features['late_slope'] = float(late_slope)\n",
        "        \n",
        "        features['slope_ratio_early_mid'] = float(early_slope / (mid_slope + 1e-10))\n",
        "        features['slope_ratio_mid_late'] = float(mid_slope / (late_slope + 1e-10))\n",
        "    else:\n",
        "        features['early_slope'] = np.nan\n",
        "        features['mid_slope'] = np.nan\n",
        "        features['late_slope'] = np.nan\n",
        "        features['slope_ratio_early_mid'] = np.nan\n",
        "        features['slope_ratio_mid_late'] = np.nan\n",
        "    \n",
        "    if n > 2:\n",
        "        second_deriv = np.diff(np.diff(stresses)) / (np.diff(strains[:-1]) + 1e-10) ** 2\n",
        "        features['avg_curvature'] = float(np.mean(second_deriv))\n",
        "        features['max_curvature'] = float(np.max(np.abs(second_deriv)))\n",
        "    else:\n",
        "        features['avg_curvature'] = np.nan\n",
        "        features['max_curvature'] = np.nan\n",
        "    \n",
        "    # === STEP 13: Stress Ratios ===\n",
        "    if 'stress_at_0.1' in features and 'stress_at_0.01' in features and not np.isnan(features['stress_at_0.1']) and not np.isnan(features['stress_at_0.01']):\n",
        "        features['stress_ratio_0.1_to_0.01'] = float(features['stress_at_0.1'] / (features['stress_at_0.01'] + 1e-10))\n",
        "    else:\n",
        "        features['stress_ratio_0.1_to_0.01'] = np.nan\n",
        "    \n",
        "    if 'stress_at_0.2' in features and 'stress_at_0.1' in features and not np.isnan(features['stress_at_0.2']) and not np.isnan(features['stress_at_0.1']):\n",
        "        features['stress_ratio_0.2_to_0.1'] = float(features['stress_at_0.2'] / (features['stress_at_0.1'] + 1e-10))\n",
        "    else:\n",
        "        features['stress_ratio_0.2_to_0.1'] = np.nan\n",
        "    \n",
        "    # === STEP 14: Detect True vs Engineering Stress-Strain ===\n",
        "    last_portion = int(len(stresses) * 0.8)\n",
        "    if last_portion < len(stresses):\n",
        "        last_stress_avg = np.mean(stresses[last_portion:])\n",
        "        peak_stress = np.max(stresses[:last_portion])\n",
        "        stress_drop_ratio = (peak_stress - last_stress_avg) / (peak_stress + 1e-10)\n",
        "        features['is_likely_engineering'] = 1.0 if stress_drop_ratio > 0.05 else 0.0\n",
        "        features['stress_drop_ratio'] = float(stress_drop_ratio)\n",
        "    else:\n",
        "        features['is_likely_engineering'] = 0.0\n",
        "        features['stress_drop_ratio'] = 0.0\n",
        "    \n",
        "    return features\n",
        "\n",
        "print(\"âœ“ Feature extraction function defined! (54 features)\")\n",
        "print(\"  Ready to extract features from curve_data in next step.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 6B: Creating DataFrame with Features + Target\n",
            "============================================================\n",
            "âœ“ DataFrame shape: (392, 84) (rows: 392, columns: 84)\n",
            "âœ“ Successfully processed: 392 curves\n",
            "\n",
            "ðŸ“‹ Column Categories:\n",
            "  ðŸŽ¯ TARGET: composition_string\n",
            "  ðŸ“Š Extracted Features: 54\n",
            "  ðŸ“ Metadata (tracking): 4\n",
            "  ðŸ”¬ Axis/Unit (reference): 3\n",
            "  âš—ï¸  Alloy composition (reference): 22\n",
            "\n",
            "ðŸ“Š Feature Categories (54 total):\n",
            "  â€¢ Basic Mechanical Properties\n",
            "  â€¢ Yield Strength\n",
            "  â€¢ Stress at Strain Points\n",
            "  â€¢ Elastic Properties\n",
            "  â€¢ Strain Hardening\n",
            "  â€¢ Work Hardening\n",
            "  â€¢ Energy Metrics\n",
            "  â€¢ Statistical\n",
            "  â€¢ Curve Shape\n",
            "  â€¢ Stress Ratios\n",
            "  â€¢ Engineering Detection\n",
            "\n",
            "ðŸ“ˆ Sample Feature Names (first 10):\n",
            "   1. avg_curvature\n",
            "   2. avg_hardening_rate\n",
            "   3. early_slope\n",
            "   4. elastic_intercept\n",
            "   5. elastic_modulus\n",
            "   6. energy_elastic_region\n",
            "   7. energy_plastic_region\n",
            "   8. hardening_rate_early\n",
            "   9. hardening_rate_late\n",
            "  10. hardening_rate_mid\n",
            "\n",
            "ðŸŽ¯ Target Variable Analysis:\n",
            "  Column: composition_string\n",
            "  Non-empty targets: 392\n",
            "  Empty targets: 10\n",
            "  Unique compositions: 78\n",
            "\n",
            "ðŸ“‹ Sample Target Values (first 5):\n",
            "  1. Co20-Cr20-Fe20-Mn20-Ni20\n",
            "  2. Co20-Cr20-Fe20-Mn20-Ni20\n",
            "  3. Co20-Cr20-Fe20-Mn20-Ni20\n",
            "  4. Co20-Cr20-Fe20-Mn20-Ni20\n",
            "  5. Co20-Cr20-Fe20-Mn20-Ni20\n",
            "\n",
            "ðŸ“ˆ DataFrame Preview:\n",
            "      curve_id        composition_string  avg_curvature  avg_hardening_rate  \\\n",
            "0   fig14a-77K  Co20-Cr20-Fe20-Mn20-Ni20   -3920.515359          657.856710   \n",
            "1  fig14a-293K  Co20-Cr20-Fe20-Mn20-Ni20   -5964.619605          360.233306   \n",
            "2  fig14a-473K  Co20-Cr20-Fe20-Mn20-Ni20   -6369.203602          342.997404   \n",
            "3  fig14a-673K  Co20-Cr20-Fe20-Mn20-Ni20   -5994.481358          449.550470   \n",
            "4  fig14a-873K  Co20-Cr20-Fe20-Mn20-Ni20   -3222.726944          247.017432   \n",
            "\n",
            "   early_slope  elastic_intercept  elastic_modulus  \n",
            "0  1494.701298         585.781808      1636.972799  \n",
            "1  1411.249999         350.447240      1564.330985  \n",
            "2  1285.157893         287.743913      1461.791521  \n",
            "3  1240.103894         261.107046      1429.241278  \n",
            "4   989.869564         246.236152      1056.710161  \n",
            "\n",
            "âœ… DataFrame ready for machine learning!\n",
            "   âœ“ Features (X): 54 columns\n",
            "   âœ“ Target (y): 'composition_string' - composition string\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/lib/python3.11/site-packages/scipy/interpolate/_interpolate.py:479: RuntimeWarning: divide by zero encountered in divide\n",
            "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
            "/opt/homebrew/lib/python3.11/site-packages/scipy/interpolate/_interpolate.py:482: RuntimeWarning: invalid value encountered in multiply\n",
            "  y_new = slope*(x_new - x_lo)[:, None] + y_lo\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# STEP 6B: Create DataFrame with features + composition string target\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 6B: Creating DataFrame with Features + Target\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Prepare the data for DataFrame creation\n",
        "df_data = []\n",
        "skipped_count = 0\n",
        "\n",
        "for curve_info in curves_list:\n",
        "    # Extract 54 features from curve_data\n",
        "    curve_data = curve_info.get('curve_data', [])\n",
        "    extracted_features = extract_features_from_curve(curve_data, debug=False)\n",
        "    \n",
        "    if extracted_features is None:\n",
        "        skipped_count += 1\n",
        "        continue\n",
        "    \n",
        "    row = {\n",
        "        # Metadata (for tracking only, not used as features)\n",
        "        'paper_title': curve_info.get('paper_title', ''),\n",
        "        'graph_id': curve_info.get('graph_id', ''),\n",
        "        'curve_id': curve_info.get('curve_id', ''),\n",
        "        'curve_label': curve_info.get('curve_label', ''),\n",
        "        \n",
        "        # Axis labels and units (for reference)\n",
        "        'x_axis_label': curve_info.get('x_axis_label', ''),\n",
        "        'y_axis_label': curve_info.get('y_axis_label', ''),\n",
        "        'y_unit': curve_info.get('y_unit', ''),\n",
        "        \n",
        "        # TARGET VARIABLE: Composition String\n",
        "        'composition_string': curve_info.get('composition_string', ''),\n",
        "    }\n",
        "    \n",
        "    # Add all 54 extracted features to the row\n",
        "    row.update(extracted_features)\n",
        "    \n",
        "    # Expand alloy composition dictionary into separate columns (for reference/analysis)\n",
        "    alloy_composition = curve_info.get('alloy_composition', {})\n",
        "    for element, percentage in alloy_composition.items():\n",
        "        # Clean element name (remove any units or extra info)\n",
        "        element_clean = element.split('(')[0].strip() if '(' in element else element.strip()\n",
        "        row[element_clean] = percentage  # Simple column name: Al, Cr, Fe, etc.\n",
        "    \n",
        "    df_data.append(row)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(df_data)\n",
        "\n",
        "print(f\"âœ“ DataFrame shape: {df.shape} (rows: {df.shape[0]}, columns: {df.shape[1]})\")\n",
        "print(f\"âœ“ Successfully processed: {len(df)} curves\")\n",
        "if skipped_count > 0:\n",
        "    print(f\"âš ï¸  Skipped {skipped_count} curves (insufficient data points)\")\n",
        "\n",
        "# Identify column categories\n",
        "metadata_cols = ['paper_title', 'graph_id', 'curve_id', 'curve_label']\n",
        "axis_cols = ['x_axis_label', 'y_axis_label', 'y_unit']\n",
        "target_col = 'composition_string'\n",
        "\n",
        "common_elements = ['Al', 'B', 'C', 'Co', 'Cr', 'Cu', 'Fe', 'Hf', 'Mg', 'Mn', 'Mo', \n",
        "                   'N', 'Nb', 'Nd', 'Ni', 'Si', 'Ta', 'Ti', 'V', 'W', 'Y', 'Zr']\n",
        "alloy_cols = [c for c in df.columns if c in common_elements]\n",
        "\n",
        "# The 54 extracted features are all other columns\n",
        "all_cols = set(df.columns)\n",
        "excluded_cols = set(metadata_cols + axis_cols + alloy_cols + [target_col])\n",
        "feature_cols = sorted([c for c in all_cols if c not in excluded_cols])\n",
        "\n",
        "print(f\"\\nðŸ“‹ Column Categories:\")\n",
        "print(f\"  ðŸŽ¯ TARGET: {target_col}\")\n",
        "print(f\"  ðŸ“Š Extracted Features: {len(feature_cols)}\")\n",
        "print(f\"  ðŸ“ Metadata (tracking): {len(metadata_cols)}\")\n",
        "print(f\"  ðŸ”¬ Axis/Unit (reference): {len(axis_cols)}\")\n",
        "print(f\"  âš—ï¸  Alloy composition (reference): {len(alloy_cols)}\")\n",
        "\n",
        "print(f\"\\nðŸ“Š Feature Categories ({len(feature_cols)} total):\")\n",
        "print(f\"  â€¢ Basic Mechanical Properties\")\n",
        "print(f\"  â€¢ Yield Strength\")\n",
        "print(f\"  â€¢ Stress at Strain Points\")\n",
        "print(f\"  â€¢ Elastic Properties\")\n",
        "print(f\"  â€¢ Strain Hardening\")\n",
        "print(f\"  â€¢ Work Hardening\")\n",
        "print(f\"  â€¢ Energy Metrics\")\n",
        "print(f\"  â€¢ Statistical\")\n",
        "print(f\"  â€¢ Curve Shape\")\n",
        "print(f\"  â€¢ Stress Ratios\")\n",
        "print(f\"  â€¢ Engineering Detection\")\n",
        "\n",
        "print(f\"\\nðŸ“ˆ Sample Feature Names (first 10):\")\n",
        "for i, feat in enumerate(feature_cols[:10], 1):\n",
        "    print(f\"  {i:2d}. {feat}\")\n",
        "\n",
        "# Check target variable\n",
        "print(f\"\\nðŸŽ¯ Target Variable Analysis:\")\n",
        "print(f\"  Column: {target_col}\")\n",
        "print(f\"  Non-empty targets: {df[target_col].notna().sum()}\")\n",
        "print(f\"  Empty targets: {df[target_col].isna().sum() + (df[target_col] == '').sum()}\")\n",
        "print(f\"  Unique compositions: {df[target_col].nunique()}\")\n",
        "\n",
        "print(f\"\\nðŸ“‹ Sample Target Values (first 5):\")\n",
        "for i, comp_str in enumerate(df[target_col].head(5), 1):\n",
        "    print(f\"  {i}. {comp_str}\")\n",
        "\n",
        "# Preview DataFrame\n",
        "print(f\"\\nðŸ“ˆ DataFrame Preview:\")\n",
        "preview_cols = ['curve_id', target_col] + feature_cols[:5]\n",
        "print(df[preview_cols].head())\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\nâœ… DataFrame ready for machine learning!\")\n",
        "print(f\"   âœ“ Features (X): {len(feature_cols)} columns\")\n",
        "print(f\"   âœ“ Target (y): '{target_col}' - composition string\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âš ï¸  Features with missing values:\n",
            "  elastic_intercept: 14 missing (3.6%)\n",
            "  elastic_modulus: 14 missing (3.6%)\n",
            "  stress_at_0.01: 141 missing (36.0%)\n",
            "  stress_at_0.02: 73 missing (18.6%)\n",
            "  stress_at_0.05: 42 missing (10.7%)\n",
            "  stress_at_0.1: 59 missing (15.1%)\n",
            "  stress_at_0.2: 136 missing (34.7%)\n",
            "  stress_at_0.3: 213 missing (54.3%)\n",
            "  stress_at_0.5: 314 missing (80.1%)\n",
            "  stress_ratio_0.1_to_0.01: 184 missing (46.9%)\n",
            "  stress_ratio_0.2_to_0.1: 141 missing (36.0%)\n",
            "  yield_strength_002: 9 missing (2.3%)\n",
            "  yield_to_uts_ratio: 9 missing (2.3%)\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values in features\n",
        "missing_features = df[feature_cols].isna().sum()\n",
        "features_with_missing = missing_features[missing_features > 0]\n",
        "if len(features_with_missing) > 0:\n",
        "    print(f\"\\nâš ï¸  Features with missing values:\")\n",
        "    for feat, count in features_with_missing.items():\n",
        "        print(f\"  {feat}: {count} missing ({count/len(df)*100:.1f}%)\")\n",
        "else:\n",
        "    print(f\"\\nâœ“ No missing values in feature columns!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "STEP 7: Data Preprocessing for Machine Learning\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "Step 7.1: Handling Null Values\n",
            "================================================================================\n",
            "âš ï¸  Found 13 features with null values:\n",
            "  stress_at_0.5                      : 314 nulls (80.10%)\n",
            "  stress_at_0.3                      : 213 nulls (54.34%)\n",
            "  stress_ratio_0.1_to_0.01           : 184 nulls (46.94%)\n",
            "  stress_at_0.01                     : 141 nulls (35.97%)\n",
            "  stress_ratio_0.2_to_0.1            : 141 nulls (35.97%)\n",
            "  stress_at_0.2                      : 136 nulls (34.69%)\n",
            "  stress_at_0.02                     :  73 nulls (18.62%)\n",
            "  stress_at_0.1                      :  59 nulls (15.05%)\n",
            "  stress_at_0.05                     :  42 nulls (10.71%)\n",
            "  elastic_intercept                  :  14 nulls ( 3.57%)\n",
            "  elastic_modulus                    :  14 nulls ( 3.57%)\n",
            "  yield_strength_002                 :   9 nulls ( 2.30%)\n",
            "  yield_to_uts_ratio                 :   9 nulls ( 2.30%)\n",
            "\n",
            "ðŸ“Š Filling null values with median...\n",
            "  âœ“ stress_at_0.5                      : Filled 314 nulls (median =     725.0168)\n",
            "  âœ“ stress_at_0.3                      : Filled 213 nulls (median =     790.0720)\n",
            "  âœ“ stress_ratio_0.1_to_0.01           : Filled 184 nulls (median =       1.5394)\n",
            "  âœ“ stress_at_0.01                     : Filled 141 nulls (median =     422.8884)\n",
            "  âœ“ stress_ratio_0.2_to_0.1            : Filled 141 nulls (median =       1.2229)\n",
            "  âœ“ stress_at_0.2                      : Filled 136 nulls (median =     744.4044)\n",
            "  âœ“ stress_at_0.02                     : Filled  73 nulls (median =     509.1612)\n",
            "  âœ“ stress_at_0.1                      : Filled  59 nulls (median =     659.9536)\n",
            "  âœ“ stress_at_0.05                     : Filled  42 nulls (median =     592.3404)\n",
            "  âœ“ elastic_intercept                  : Filled  14 nulls (median =     249.1878)\n",
            "  âœ“ elastic_modulus                    : Filled  14 nulls (median =    3818.8619)\n",
            "  âœ“ yield_strength_002                 : Filled   9 nulls (median =     329.4470)\n",
            "  âœ“ yield_to_uts_ratio                 : Filled   9 nulls (median =       0.3938)\n",
            "\n",
            "âœ… Filled 1349 total null values\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/xy/0xhnpbcd2fsby1z0rgb21v_40000gn/T/ipykernel_41573/920556588.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[feature].fillna(median_value, inplace=True)\n",
            "/var/folders/xy/0xhnpbcd2fsby1z0rgb21v_40000gn/T/ipykernel_41573/920556588.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[feature].fillna(median_value, inplace=True)\n",
            "/var/folders/xy/0xhnpbcd2fsby1z0rgb21v_40000gn/T/ipykernel_41573/920556588.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[feature].fillna(median_value, inplace=True)\n",
            "/var/folders/xy/0xhnpbcd2fsby1z0rgb21v_40000gn/T/ipykernel_41573/920556588.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[feature].fillna(median_value, inplace=True)\n",
            "/var/folders/xy/0xhnpbcd2fsby1z0rgb21v_40000gn/T/ipykernel_41573/920556588.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[feature].fillna(median_value, inplace=True)\n",
            "/var/folders/xy/0xhnpbcd2fsby1z0rgb21v_40000gn/T/ipykernel_41573/920556588.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[feature].fillna(median_value, inplace=True)\n",
            "/var/folders/xy/0xhnpbcd2fsby1z0rgb21v_40000gn/T/ipykernel_41573/920556588.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[feature].fillna(median_value, inplace=True)\n",
            "/var/folders/xy/0xhnpbcd2fsby1z0rgb21v_40000gn/T/ipykernel_41573/920556588.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[feature].fillna(median_value, inplace=True)\n",
            "/var/folders/xy/0xhnpbcd2fsby1z0rgb21v_40000gn/T/ipykernel_41573/920556588.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[feature].fillna(median_value, inplace=True)\n",
            "/var/folders/xy/0xhnpbcd2fsby1z0rgb21v_40000gn/T/ipykernel_41573/920556588.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[feature].fillna(median_value, inplace=True)\n",
            "/var/folders/xy/0xhnpbcd2fsby1z0rgb21v_40000gn/T/ipykernel_41573/920556588.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[feature].fillna(median_value, inplace=True)\n",
            "/var/folders/xy/0xhnpbcd2fsby1z0rgb21v_40000gn/T/ipykernel_41573/920556588.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[feature].fillna(median_value, inplace=True)\n",
            "/var/folders/xy/0xhnpbcd2fsby1z0rgb21v_40000gn/T/ipykernel_41573/920556588.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[feature].fillna(median_value, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# STEP 7: Handle null/missing values and prepare for ML\n",
        "# Fill null values with median, then prepare features and targets\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"STEP 7: Data Preprocessing for Machine Learning\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Step 7.1: Handling Null Values\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "null_counts = df[feature_cols].isnull().sum()\n",
        "features_with_nulls = null_counts[null_counts > 0].sort_values(ascending=False)\n",
        "\n",
        "if len(features_with_nulls) > 0:\n",
        "    print(f\"âš ï¸  Found {len(features_with_nulls)} features with null values:\")\n",
        "    for feature, count in features_with_nulls.items():\n",
        "        percentage = (count / len(df)) * 100\n",
        "        print(f\"  {feature:35s}: {count:3d} nulls ({percentage:5.2f}%)\")\n",
        "    \n",
        "    print(\"\\nðŸ“Š Filling null values with median...\")\n",
        "    filled_count = 0\n",
        "    for feature in features_with_nulls.index:\n",
        "        median_value = df[feature].median()\n",
        "        null_count = df[feature].isnull().sum()\n",
        "        df[feature].fillna(median_value, inplace=True)\n",
        "        filled_count += null_count\n",
        "        print(f\"  âœ“ {feature:35s}: Filled {null_count:3d} nulls (median = {median_value:12.4f})\")\n",
        "    \n",
        "    print(f\"\\nâœ… Filled {filled_count} total null values\")\n",
        "else:\n",
        "    print(\"âœ… No null values found in feature columns!\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 8: Separating Features (X) and Target (y)\n",
            "============================================================\n",
            "âœ“ Features (X):\n",
            "  Shape: (392, 54)\n",
            "  Columns: 54 features\n",
            "  Data type: {dtype('float64'): 53, dtype('int64'): 1}\n",
            "\n",
            "âœ“ Target (y):\n",
            "  Shape: (392,)\n",
            "  Column: 'composition_string'\n",
            "  Unique values: 78\n",
            "  Non-empty: 382\n",
            "  Empty: 10\n",
            "\n",
            "ðŸ” Data Quality Check:\n",
            "  Missing values in X: 0\n",
            "  Missing values in y: 0\n",
            "  Empty strings in y: 10\n",
            "  Infinite values in X: 0\n",
            "\n",
            "âš ï¸  Removing 10 rows with empty/null target values...\n",
            "  New X shape: (382, 54)\n",
            "  New y shape: (382,)\n",
            "\n",
            "ðŸ“Š Sample Data:\n",
            "\n",
            "First 5 rows of X (features):\n",
            "   avg_curvature  avg_hardening_rate  early_slope  elastic_intercept  \\\n",
            "0   -3920.515359          657.856710  1494.701298         585.781808   \n",
            "1   -5964.619605          360.233306  1411.249999         350.447240   \n",
            "2   -6369.203602          342.997404  1285.157893         287.743913   \n",
            "3   -5994.481358          449.550470  1240.103894         261.107046   \n",
            "4   -3222.726944          247.017432   989.869564         246.236152   \n",
            "\n",
            "   elastic_modulus  energy_elastic_region  energy_plastic_region  \\\n",
            "0      1636.972799              74.912860             772.730413   \n",
            "1      1564.330985              27.968708             304.526787   \n",
            "2      1461.791521              15.464852             171.697273   \n",
            "3      1429.241278               9.861891             136.907975   \n",
            "4      1056.710161              17.251478             165.897068   \n",
            "\n",
            "   hardening_rate_early  hardening_rate_late  hardening_rate_mid  ...  \\\n",
            "0           1422.451152           -22.276148          615.903428  ...   \n",
            "1           1306.060496          -608.690049          471.413412  ...   \n",
            "2           1293.994504          -728.357259          569.021310  ...   \n",
            "3           1189.112033          -338.757200          590.741771  ...   \n",
            "4            969.511569          -444.028937          287.819077  ...   \n",
            "\n",
            "   stress_median  stress_range  stress_ratio_0.1_to_0.01  \\\n",
            "0      1012.0225       523.486                  1.539432   \n",
            "1       596.3680       296.248                  1.362595   \n",
            "2       482.7010       233.660                  1.383291   \n",
            "3       442.0010       212.018                  1.375203   \n",
            "4       390.5640       174.930                  1.329314   \n",
            "\n",
            "   stress_ratio_0.2_to_0.1  stress_std   toughness  ultimate_tensile_strength  \\\n",
            "0                 1.146965  161.909318  860.178906                   1115.815   \n",
            "1                 1.159708   88.579577  339.644315                    655.557   \n",
            "2                 1.178392   71.720781  193.599456                    534.161   \n",
            "3                 1.183109   67.167297  152.062866                    488.563   \n",
            "4                 1.160218   53.438365  187.962324                    423.189   \n",
            "\n",
            "   uts_strain  yield_strength_002  yield_to_uts_ratio  \n",
            "0       0.828             592.329            0.530849  \n",
            "1       0.495             359.309            0.548097  \n",
            "2       0.365             300.501            0.562566  \n",
            "3       0.334             276.545            0.566038  \n",
            "4       0.343             248.259            0.586639  \n",
            "\n",
            "[5 rows x 54 columns]\n",
            "\n",
            "First 10 target values (y):\n",
            "   1. Co20-Cr20-Fe20-Mn20-Ni20\n",
            "   2. Co20-Cr20-Fe20-Mn20-Ni20\n",
            "   3. Co20-Cr20-Fe20-Mn20-Ni20\n",
            "   4. Co20-Cr20-Fe20-Mn20-Ni20\n",
            "   5. Co20-Cr20-Fe20-Mn20-Ni20\n",
            "   6. Co20-Cr20-Fe20-Mn20-Ni20\n",
            "   7. Al2.44-Co24.39-Cr24.39-Fe24.39-Ni24.39\n",
            "   8. Al2.44-Co24.39-Cr24.39-Fe24.39-Ni24.39\n",
            "   9. Al2.44-Co24.39-Cr24.39-Fe24.39-Ni24.39\n",
            "  10. Co20-Cr20-Fe20-Mn20-Ni20\n",
            "\n",
            "ðŸ“ˆ Feature Statistics:\n",
            "                               mean           std           min           max\n",
            "avg_curvature          1.852460e+19  3.399263e+20 -2.649007e+21  2.589256e+21\n",
            "avg_hardening_rate     1.468270e+10  4.606401e+10 -4.265542e+10  3.319169e+11\n",
            "early_slope            1.024028e+10  2.001441e+11 -3.250000e+02  3.911780e+12\n",
            "elastic_intercept      2.496240e+02  7.254180e+02 -1.039262e+04  1.494742e+03\n",
            "elastic_modulus        1.542007e+04  3.820315e+04 -3.414300e+04  5.006929e+05\n",
            "energy_elastic_region  3.857266e+01  2.665160e+02  0.000000e+00  4.178093e+03\n",
            "energy_plastic_region  5.714101e+02  4.871675e+03  4.599816e+00  8.929583e+04\n",
            "hardening_rate_early   3.098023e+10  8.546673e+10 -4.045936e+10  6.878632e+11\n",
            "hardening_rate_late    2.937465e+09  3.468101e+10 -1.279663e+11  3.789827e+11\n",
            "hardening_rate_mid     1.127384e+10  6.035596e+10 -2.849664e+11  4.967691e+11\n",
            "\n",
            "âœ… Data ready for machine learning!\n",
            "   X (features): 382 samples Ã— 54 features\n",
            "   y (target): 382 composition strings\n",
            "   Ready for train/test split!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# STEP 8: Separate Features (X) and Target (y)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 8: Separating Features (X) and Target (y)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = df[feature_cols].copy()\n",
        "y = df['composition_string'].copy()\n",
        "\n",
        "print(f\"âœ“ Features (X):\")\n",
        "print(f\"  Shape: {X.shape}\")\n",
        "print(f\"  Columns: {len(feature_cols)} features\")\n",
        "print(f\"  Data type: {X.dtypes.value_counts().to_dict()}\")\n",
        "\n",
        "print(f\"\\nâœ“ Target (y):\")\n",
        "print(f\"  Shape: {y.shape}\")\n",
        "print(f\"  Column: 'composition_string'\")\n",
        "print(f\"  Unique values: {y.nunique()}\")\n",
        "print(f\"  Non-empty: {(y != '').sum()}\")\n",
        "print(f\"  Empty: {(y == '').sum()}\")\n",
        "\n",
        "# Check for any remaining issues\n",
        "print(f\"\\nðŸ” Data Quality Check:\")\n",
        "print(f\"  Missing values in X: {X.isnull().sum().sum()}\")\n",
        "print(f\"  Missing values in y: {y.isnull().sum()}\")\n",
        "print(f\"  Empty strings in y: {(y == '').sum()}\")\n",
        "print(f\"  Infinite values in X: {np.isinf(X.values).sum()}\")\n",
        "\n",
        "# Remove rows with empty composition strings (if any)\n",
        "valid_mask = (y != '') & (y.notna())\n",
        "if valid_mask.sum() < len(y):\n",
        "    removed = len(y) - valid_mask.sum()\n",
        "    print(f\"\\nâš ï¸  Removing {removed} rows with empty/null target values...\")\n",
        "    X = X[valid_mask].copy()\n",
        "    y = y[valid_mask].copy()\n",
        "    print(f\"  New X shape: {X.shape}\")\n",
        "    print(f\"  New y shape: {y.shape}\")\n",
        "\n",
        "# Show sample data\n",
        "print(f\"\\nðŸ“Š Sample Data:\")\n",
        "print(f\"\\nFirst 5 rows of X (features):\")\n",
        "print(X.head())\n",
        "\n",
        "print(f\"\\nFirst 10 target values (y):\")\n",
        "for i, comp in enumerate(y.head(10), 1):\n",
        "    print(f\"  {i:2d}. {comp}\")\n",
        "\n",
        "# Summary statistics\n",
        "print(f\"\\nðŸ“ˆ Feature Statistics:\")\n",
        "print(X.describe().T[['mean', 'std', 'min', 'max']].head(10))\n",
        "\n",
        "print(f\"\\nâœ… Data ready for machine learning!\")\n",
        "print(f\"   X (features): {X.shape[0]} samples Ã— {X.shape[1]} features\")\n",
        "print(f\"   y (target): {y.shape[0]} composition strings\")\n",
        "print(f\"   Ready for train/test split!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 9: Train/Test Split (80/20)\n",
            "============================================================\n",
            "âœ“ Train/Test Split Complete:\n",
            "\n",
            "ðŸ“Š Training Set:\n",
            "  X_train shape: (305, 54)\n",
            "  y_train shape: (305,)\n",
            "  Samples: 305 (79.8%)\n",
            "  Unique compositions: 74\n",
            "\n",
            "ðŸ“Š Test Set:\n",
            "  X_test shape: (77, 54)\n",
            "  y_test shape: (77,)\n",
            "  Samples: 77 (20.2%)\n",
            "  Unique compositions: 32\n",
            "\n",
            "ðŸ“‹ Sample Training Data:\n",
            "\n",
            "First 5 training samples (features):\n",
            "     avg_curvature  avg_hardening_rate   early_slope  elastic_intercept  \\\n",
            "171  -2.013761e+04        2.257324e+03   4931.857131         537.003084   \n",
            "330   4.240000e+19        2.565817e+10  24347.999513         255.013316   \n",
            "225  -3.891944e+19        1.563723e+10   5665.826659         472.228329   \n",
            "100   8.116667e+18        1.094302e+10  20711.230610          76.243714   \n",
            "237   1.325250e+20        6.706886e+10  85230.495738          32.050000   \n",
            "\n",
            "     elastic_modulus  energy_elastic_region  energy_plastic_region  \\\n",
            "171      5306.424741              15.337696             184.448393   \n",
            "330     23526.263158               1.442143              48.846864   \n",
            "225      6848.430033              27.590844             689.974787   \n",
            "100     23134.464286               3.046053              91.600719   \n",
            "237     57068.705882               0.412697              18.531290   \n",
            "\n",
            "     hardening_rate_early  hardening_rate_late  hardening_rate_mid  ...  \\\n",
            "171          4.612091e+03           702.520829        1.719002e+03  ...   \n",
            "330          8.125087e+10          2260.290080        1.382486e+04  ...   \n",
            "225          4.821479e+10           314.978841        1.192046e+03  ...   \n",
            "100          3.392336e+10          4019.887605        6.655674e+03  ...   \n",
            "237          1.348779e+11          5581.403472        7.310961e+10  ...   \n",
            "\n",
            "     stress_median  stress_range  stress_ratio_0.1_to_0.01  \\\n",
            "171       908.7160       474.862                  1.488426   \n",
            "330       650.0440       371.535                  1.539432   \n",
            "225      1192.5070       876.773                  1.539432   \n",
            "100       836.7605       799.612                  1.539432   \n",
            "237       482.5120       544.136                  1.539432   \n",
            "\n",
            "     stress_ratio_0.2_to_0.1  stress_std   toughness  \\\n",
            "171                 1.139829  145.099007  205.405901   \n",
            "330                 1.222900  116.551561   51.336038   \n",
            "225                 1.201799  260.656232  726.523256   \n",
            "100                 1.222900  246.013032   95.718634   \n",
            "237                 1.222900  171.198630   18.943987   \n",
            "\n",
            "     ultimate_tensile_strength  uts_strain  yield_strength_002  \\\n",
            "171                   1019.700       0.228             544.838   \n",
            "330                    807.550       0.079             436.015   \n",
            "225                   1364.563       0.629             487.790   \n",
            "100                   1145.657       0.120             346.045   \n",
            "237                    673.580       0.036             329.447   \n",
            "\n",
            "     yield_to_uts_ratio  \n",
            "171            0.534312  \n",
            "330            0.539923  \n",
            "225            0.357470  \n",
            "100            0.302049  \n",
            "237            0.393846  \n",
            "\n",
            "[5 rows x 54 columns]\n",
            "\n",
            "First 10 training targets:\n",
            "   1. Co19.1-Cr14.8-Fe41.8-Mn18.8-Si5.5\n",
            "   2. Co23.78-Cr23.78-Fe23.78-Nb4.88-Ni23.78\n",
            "   3. Co10-Cr15-Fe42-Mn28-Si5\n",
            "   4. Al16.39-Co16.39-Cr16.39-Fe16.39-Ni34.44\n",
            "   5. Al14.9-Co16.2-Cr18.1-Fe17.3-Ni32.8-Ti0.7\n",
            "   6. Al25-Nb25-Ti25-V25\n",
            "   7. B0-Co10-Cr10-Fe40-Mn40\n",
            "   8. Al7.5-Cr22.5-Fe35-Mn20-Ni15\n",
            "   9. Al6.1-B0.03-C0.2-Fe58.4-Ni32.6-Ti2.9-Zr0.1\n",
            "  10. Co10-Cr10-Fe50-Mn30\n",
            "\n",
            "ðŸ“‹ Sample Test Data:\n",
            "\n",
            "First 5 test targets:\n",
            "   1. Mo20-Nb20-Ta20-V20-W20\n",
            "   2. Al9-Co18.2-Cr18.2-Cu18.2-Fe18.2-Ni18.2\n",
            "   3. Co10-Cr15-Fe42-Mn28-Si5\n",
            "   4. Co23.26-Cr23.26-Fe23.26-Mo6.96-Ni23.26\n",
            "   5. Co20-Cr15-Cu1.5-Fe38.5-Mn20-Si5\n",
            "\n",
            "âœ… Data split complete and ready for model training!\n",
            "   Training: 305 samples\n",
            "   Testing: 77 samples\n",
            "   Features: 54\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# STEP 9: Train/Test Split (80/20)\n",
        "# =============================================================================\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 9: Train/Test Split (80/20)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Split the data: 80% training, 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.2, \n",
        "    random_state=42,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"âœ“ Train/Test Split Complete:\")\n",
        "print(f\"\\nðŸ“Š Training Set:\")\n",
        "print(f\"  X_train shape: {X_train.shape}\")\n",
        "print(f\"  y_train shape: {y_train.shape}\")\n",
        "print(f\"  Samples: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "print(f\"  Unique compositions: {y_train.nunique()}\")\n",
        "\n",
        "print(f\"\\nðŸ“Š Test Set:\")\n",
        "print(f\"  X_test shape: {X_test.shape}\")\n",
        "print(f\"  y_test shape: {y_test.shape}\")\n",
        "print(f\"  Samples: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "print(f\"  Unique compositions: {y_test.nunique()}\")\n",
        "\n",
        "# # Check for data leakage (compositions in test that aren't in train)\n",
        "# train_compositions = set(y_train.unique())\n",
        "# test_compositions = set(y_test.unique())\n",
        "# new_compositions_in_test = test_compositions - train_compositions\n",
        "\n",
        "# print(f\"\\nðŸ” Composition Distribution:\")\n",
        "# print(f\"  Compositions in training: {len(train_compositions)}\")\n",
        "# print(f\"  Compositions in test: {len(test_compositions)}\")\n",
        "# print(f\"  New compositions in test (not in train): {len(new_compositions_in_test)}\")\n",
        "\n",
        "# if len(new_compositions_in_test) > 0:\n",
        "#     print(f\"\\nâš ï¸  Warning: {len(new_compositions_in_test)} compositions appear in test but not in training\")\n",
        "#     print(f\"  This is expected for compositional prediction tasks\")\n",
        "#     if len(new_compositions_in_test) <= 10:\n",
        "#         print(f\"  Examples:\")\n",
        "#         for comp in list(new_compositions_in_test)[:10]:\n",
        "#             print(f\"    â€¢ {comp}\")\n",
        "\n",
        "# Show sample from training set\n",
        "print(f\"\\nðŸ“‹ Sample Training Data:\")\n",
        "print(f\"\\nFirst 5 training samples (features):\")\n",
        "print(X_train.head())\n",
        "\n",
        "print(f\"\\nFirst 10 training targets:\")\n",
        "for i, (idx, comp) in enumerate(y_train.head(10).items(), 1):\n",
        "    print(f\"  {i:2d}. {comp}\")\n",
        "\n",
        "# Show sample from test set\n",
        "print(f\"\\nðŸ“‹ Sample Test Data:\")\n",
        "print(f\"\\nFirst 5 test targets:\")\n",
        "for i, (idx, comp) in enumerate(y_test.head(5).items(), 1):\n",
        "    print(f\"  {i:2d}. {comp}\")\n",
        "\n",
        "print(f\"\\nâœ… Data split complete and ready for model training!\")\n",
        "print(f\"   Training: {len(X_train)} samples\")\n",
        "print(f\"   Testing: {len(X_test)} samples\")\n",
        "print(f\"   Features: {X_train.shape[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 10: Feature Scaling (StandardScaler)\n",
            "============================================================\n",
            "âœ“ Scaling Complete:\n",
            "\n",
            "ðŸ“Š Training Set (Scaled):\n",
            "  Shape: (305, 54)\n",
            "  Mean: ~-0.000000 (should be ~0)\n",
            "  Std: ~1.001643 (should be ~1)\n",
            "\n",
            "ðŸ“Š Test Set (Scaled):\n",
            "  Shape: (77, 54)\n",
            "  Mean: ~-0.064571\n",
            "  Std: ~0.626254\n",
            "\n",
            "ðŸ“ˆ Before/After Comparison (feature: 'avg_curvature'):\n",
            "\n",
            "  BEFORE scaling (training set):\n",
            "    Mean: 10245509944497967104.0000\n",
            "    Std:  343238415231287099392.0000\n",
            "    Min:  -2649007142857117007872.0000\n",
            "    Max:  2265076744186033078272.0000\n",
            "\n",
            "  AFTER scaling (training set):\n",
            "    Mean: 0.0000\n",
            "    Std:  1.0016\n",
            "    Min:  -7.7603\n",
            "    Max:  6.5801\n",
            "\n",
            "ðŸ“‹ Sample Scaled Training Data (first 5 rows, first 5 features):\n",
            "     avg_curvature  avg_hardening_rate  early_slope  elastic_intercept  \\\n",
            "171      -0.029899           -0.317487    -0.057354           0.373383   \n",
            "330       0.093834            0.216487    -0.057354           0.007293   \n",
            "225      -0.143474            0.007941    -0.057354           0.289290   \n",
            "100      -0.006212           -0.089751    -0.057354          -0.224792   \n",
            "237       0.356838            1.078288    -0.057354          -0.282166   \n",
            "\n",
            "     elastic_modulus  \n",
            "171        -0.265257  \n",
            "330         0.213536  \n",
            "225        -0.224735  \n",
            "100         0.203240  \n",
            "237         1.094986  \n",
            "\n",
            "ðŸ” Data Quality Check After Scaling:\n",
            "  Missing values in X_train_scaled: 0\n",
            "  Missing values in X_test_scaled: 0\n",
            "  Infinite values in X_train_scaled: 0\n",
            "  Infinite values in X_test_scaled: 0\n",
            "\n",
            "ðŸ“Š Scaled Feature Statistics (Training Set):\n",
            "                               mean       std        min        max\n",
            "avg_curvature          1.747236e-17  1.001643  -7.760270   6.580082\n",
            "avg_hardening_rate     0.000000e+00  1.001643  -1.205192   6.590063\n",
            "early_slope            0.000000e+00  1.001643  -0.057354  17.435596\n",
            "elastic_intercept      8.736181e-18  1.001643 -13.815853   1.289948\n",
            "elastic_modulus        5.824121e-17  1.001643  -1.301935  12.752831\n",
            "energy_elastic_region  2.912060e-18  1.001643  -0.148229  13.895303\n",
            "energy_plastic_region -1.747236e-17  1.001643  -0.121194  16.292514\n",
            "hardening_rate_early   2.184045e-17  1.001643  -0.904764   5.646614\n",
            "hardening_rate_late    0.000000e+00  1.001643  -3.606844  10.372202\n",
            "hardening_rate_mid    -2.912060e-17  1.001643  -1.805908   7.546466\n",
            "\n",
            "âœ… Feature scaling complete!\n",
            "   âœ“ X_train_scaled: (305, 54)\n",
            "   âœ“ X_test_scaled: (77, 54)\n",
            "   âœ“ All features normalized (meanâ‰ˆ0, stdâ‰ˆ1)\n",
            "   âœ“ Ready for model training!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# STEP 10: Feature Scaling using StandardScaler\n",
        "# =============================================================================\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 10: Feature Scaling (StandardScaler)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on training data and transform both train and test\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert back to DataFrame to preserve column names\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train.index)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test.index)\n",
        "\n",
        "print(f\"âœ“ Scaling Complete:\")\n",
        "print(f\"\\nðŸ“Š Training Set (Scaled):\")\n",
        "print(f\"  Shape: {X_train_scaled.shape}\")\n",
        "print(f\"  Mean: ~{X_train_scaled.mean().mean():.6f} (should be ~0)\")\n",
        "print(f\"  Std: ~{X_train_scaled.std().mean():.6f} (should be ~1)\")\n",
        "\n",
        "print(f\"\\nðŸ“Š Test Set (Scaled):\")\n",
        "print(f\"  Shape: {X_test_scaled.shape}\")\n",
        "print(f\"  Mean: ~{X_test_scaled.mean().mean():.6f}\")\n",
        "print(f\"  Std: ~{X_test_scaled.std().mean():.6f}\")\n",
        "\n",
        "# Show before/after comparison for first feature\n",
        "sample_feature = feature_cols[0]\n",
        "print(f\"\\nðŸ“ˆ Before/After Comparison (feature: '{sample_feature}'):\")\n",
        "print(f\"\\n  BEFORE scaling (training set):\")\n",
        "print(f\"    Mean: {X_train[sample_feature].mean():.4f}\")\n",
        "print(f\"    Std:  {X_train[sample_feature].std():.4f}\")\n",
        "print(f\"    Min:  {X_train[sample_feature].min():.4f}\")\n",
        "print(f\"    Max:  {X_train[sample_feature].max():.4f}\")\n",
        "\n",
        "print(f\"\\n  AFTER scaling (training set):\")\n",
        "print(f\"    Mean: {X_train_scaled[sample_feature].mean():.4f}\")\n",
        "print(f\"    Std:  {X_train_scaled[sample_feature].std():.4f}\")\n",
        "print(f\"    Min:  {X_train_scaled[sample_feature].min():.4f}\")\n",
        "print(f\"    Max:  {X_train_scaled[sample_feature].max():.4f}\")\n",
        "\n",
        "# Show sample of scaled data\n",
        "print(f\"\\nðŸ“‹ Sample Scaled Training Data (first 5 rows, first 5 features):\")\n",
        "print(X_train_scaled[feature_cols[:5]].head())\n",
        "\n",
        "# Verify no missing values after scaling\n",
        "print(f\"\\nðŸ” Data Quality Check After Scaling:\")\n",
        "print(f\"  Missing values in X_train_scaled: {X_train_scaled.isnull().sum().sum()}\")\n",
        "print(f\"  Missing values in X_test_scaled: {X_test_scaled.isnull().sum().sum()}\")\n",
        "print(f\"  Infinite values in X_train_scaled: {np.isinf(X_train_scaled.values).sum()}\")\n",
        "print(f\"  Infinite values in X_test_scaled: {np.isinf(X_test_scaled.values).sum()}\")\n",
        "\n",
        "# Statistics for all features after scaling\n",
        "print(f\"\\nðŸ“Š Scaled Feature Statistics (Training Set):\")\n",
        "scaling_stats = pd.DataFrame({\n",
        "    'mean': X_train_scaled.mean(),\n",
        "    'std': X_train_scaled.std(),\n",
        "    'min': X_train_scaled.min(),\n",
        "    'max': X_train_scaled.max()\n",
        "})\n",
        "print(scaling_stats.head(10))\n",
        "\n",
        "print(f\"\\nâœ… Feature scaling complete!\")\n",
        "print(f\"   âœ“ X_train_scaled: {X_train_scaled.shape}\")\n",
        "print(f\"   âœ“ X_test_scaled: {X_test_scaled.shape}\")\n",
        "print(f\"   âœ“ All features normalized (meanâ‰ˆ0, stdâ‰ˆ1)\")\n",
        "print(f\"   âœ“ Ready for model training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 11: Random Forest Classifier Training\n",
            "============================================================\n",
            "\n",
            "ðŸŒ² Initializing Random Forest Classifier...\n",
            "\n",
            "ðŸ“‹ Model Configuration:\n",
            "  Number of trees: 100\n",
            "  Max depth: None\n",
            "  Min samples split: 2\n",
            "  Min samples leaf: 1\n",
            "  Random state: 42\n",
            "\n",
            "ðŸš€ Training Random Forest on 305 samples...\n",
            "   Features: 54\n",
            "   Unique target classes: 74\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
            "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Training Complete!\n",
            "   Time taken: 0.22 seconds (0.00 minutes)\n",
            "\n",
            "ðŸ“Š Evaluating on Training Set...\n",
            "   Training Accuracy: 100.00%\n",
            "   Correct predictions: 305 / 305\n",
            "\n",
            "ðŸ“Š Evaluating on Test Set...\n",
            "   Test Accuracy: 45.45%\n",
            "   Correct predictions: 35 / 77\n",
            "\n",
            "ðŸ” Sample Predictions (First 10 from Test Set):\n",
            "#   Actual                                   Predicted                                Match \n",
            "===============================================================================================\n",
            "1   Mo20-Nb20-Ta20-V20-W20                   Mo20-Nb20-Ta20-V20-W20                   âœ“     \n",
            "2   Al9-Co18.2-Cr18.2-Cu18.2-Fe18.2-Ni18.2   Al9-Co18.2-Cr18.2-Cu18.2-Fe18.2-Ni18.2   âœ“     \n",
            "3   Co10-Cr15-Fe42-Mn28-Si5                  Co10-Cr15-Fe42-Mn28-Si5                  âœ“     \n",
            "4   Co23.26-Cr23.26-Fe23.26-Mo6.96-Ni23.26   Co19.1-Cr14.8-Fe41.8-Mn18.8-Si5.5        âœ—     \n",
            "5   Co20-Cr15-Cu1.5-Fe38.5-Mn20-Si5          Co10-Cr15-Fe42-Mn28-Si5                  âœ—     \n",
            "6   Al25-Nb25-Ti25-V25                       Al16.39-Co16.39-Cr16.39-Fe16.39-Ni34.43  âœ—     \n",
            "7   Co10-Cr10-Fe50-Mn30                      Co19.1-Cr14.8-Fe41.8-Mn18.8-Si5.5        âœ—     \n",
            "8   Co33.33-Cr33.33-Ni33.33                  Co33.33-Cr33.33-Ni33.34                  âœ—     \n",
            "9   Co25-Cr25-Fe25-Ni25                      Co25-Cr25-Fe25-Ni25                      âœ“     \n",
            "10  Al16.39-Co16.39-Cr16.39-Fe16.39-Ni34.43  Co1-Cr21.5-Fe2.5-Mn0.5-Mo9-Ni61-Si0.5    âœ—     \n",
            "\n",
            "ðŸ“ˆ Top 10 Most Important Features:\n",
            "  ultimate_tensile_strength          : 0.029459\n",
            "  strain_range                       : 0.028468\n",
            "  stress_75_percentile               : 0.028365\n",
            "  stress_at_0.02                     : 0.026572\n",
            "  max_strain                         : 0.026442\n",
            "  stress_at_0.01                     : 0.026028\n",
            "  strain_std                         : 0.025206\n",
            "  strain_mean                        : 0.025152\n",
            "  resilience                         : 0.024197\n",
            "  uts_strain                         : 0.023822\n",
            "\n",
            "============================================================\n",
            "ðŸ“Š Model Performance Summary\n",
            "============================================================\n",
            "  Training Accuracy: 100.00%\n",
            "  Test Accuracy: 45.45%\n",
            "  Overfitting: 54.55%\n",
            "  Training samples: 305\n",
            "  Test samples: 77\n",
            "  Total classes: 74\n",
            "  Training time: 0.22 seconds\n",
            "\n",
            "âœ… Random Forest model trained successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# STEP 11: Train Random Forest Classifier\n",
        "# =============================================================================\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import time\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 11: Random Forest Classifier Training\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "print(\"\\nðŸŒ² Initializing Random Forest Classifier...\")\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,        # Number of trees\n",
        "    max_depth=None,          # No limit on depth\n",
        "    min_samples_split=2,     # Minimum samples to split a node\n",
        "    min_samples_leaf=1,      # Minimum samples at leaf node\n",
        "    random_state=42,         # For reproducibility\n",
        "    n_jobs=-1,               # Use all CPU cores\n",
        "    verbose=1                # Show progress\n",
        ")\n",
        "\n",
        "print(f\"\\nðŸ“‹ Model Configuration:\")\n",
        "print(f\"  Number of trees: {rf_model.n_estimators}\")\n",
        "print(f\"  Max depth: {rf_model.max_depth}\")\n",
        "print(f\"  Min samples split: {rf_model.min_samples_split}\")\n",
        "print(f\"  Min samples leaf: {rf_model.min_samples_leaf}\")\n",
        "print(f\"  Random state: {rf_model.random_state}\")\n",
        "\n",
        "# Train the model\n",
        "print(f\"\\nðŸš€ Training Random Forest on {len(X_train_scaled)} samples...\")\n",
        "print(f\"   Features: {X_train_scaled.shape[1]}\")\n",
        "print(f\"   Unique target classes: {y_train.nunique()}\")\n",
        "\n",
        "start_time = time.time()\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nâœ… Training Complete!\")\n",
        "print(f\"   Time taken: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
        "\n",
        "# Make predictions on training set\n",
        "print(f\"\\nðŸ“Š Evaluating on Training Set...\")\n",
        "y_train_pred = rf_model.predict(X_train_scaled)\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "\n",
        "print(f\"   Training Accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"   Correct predictions: {(y_train == y_train_pred).sum()} / {len(y_train)}\")\n",
        "\n",
        "# Make predictions on test set\n",
        "print(f\"\\nðŸ“Š Evaluating on Test Set...\")\n",
        "y_test_pred = rf_model.predict(X_test_scaled)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"   Test Accuracy: {test_accuracy*100:.2f}%\")\n",
        "print(f\"   Correct predictions: {(y_test == y_test_pred).sum()} / {len(y_test)}\")\n",
        "\n",
        "# Show sample predictions\n",
        "print(f\"\\nðŸ” Sample Predictions (First 10 from Test Set):\")\n",
        "print(f\"{'#':<3} {'Actual':<40} {'Predicted':<40} {'Match':<6}\")\n",
        "print(\"=\"*95)\n",
        "for i, (actual, predicted) in enumerate(zip(y_test.head(10), y_test_pred[:10]), 1):\n",
        "    match = \"âœ“\" if actual == predicted else \"âœ—\"\n",
        "    print(f\"{i:<3} {actual:<40} {predicted:<40} {match:<6}\")\n",
        "\n",
        "# Feature importance\n",
        "print(f\"\\nðŸ“ˆ Top 10 Most Important Features:\")\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': feature_cols,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "for i, row in feature_importance.head(10).iterrows():\n",
        "    print(f\"  {row['feature']:<35s}: {row['importance']:.6f}\")\n",
        "\n",
        "# Model summary\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ“Š Model Performance Summary\")\n",
        "print(\"=\"*60)\n",
        "print(f\"  Training Accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"  Test Accuracy: {test_accuracy*100:.2f}%\")\n",
        "print(f\"  Overfitting: {(train_accuracy - test_accuracy)*100:.2f}%\")\n",
        "print(f\"  Training samples: {len(X_train_scaled)}\")\n",
        "print(f\"  Test samples: {len(X_test_scaled)}\")\n",
        "print(f\"  Total classes: {y_train.nunique()}\")\n",
        "print(f\"  Training time: {training_time:.2f} seconds\")\n",
        "\n",
        "print(f\"\\nâœ… Random Forest model trained successfully!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.11.14 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "vscode": {
      "interpreter": {
        "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
